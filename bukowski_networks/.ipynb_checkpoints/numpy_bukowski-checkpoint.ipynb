{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Bukowski\n",
    "Following Denny's deep neural network tutorial on [WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/), I try to build a  Recurrent Neural Network that can write poems like Bukowski. For this I take a broad sample of his poems (1363) and use them as input data. On this initial section all the implementation is in numpy, and the goal is to generate a couple of sentences that look like Buk's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import libaries.\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "First step is to read the poem files, store them in a convenient location (pandas DF) and keep some labeled information that might be useful in the future. The words on the lines of the poems will be tokenized, and each word will receive an index value more easily consumable by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file Bukowski_1955_73.txt\n",
      "Processing file Bukowski_1960_95.txt\n",
      "Processing file Bukowski_1972.txt\n",
      "Processing file Bukowski_1974_77.txt\n",
      "Processing file Bukowski_1960_97.txt\n",
      "Processing file Bukowski_1960_99.txt\n",
      "Processing file Bukowski_1960_97(2).txt\n",
      "Processing file Bukowski_1981_84.txt\n",
      "Processing file Bukowski_1986.txt\n",
      "Processing file Bukowski_1960_96.txt\n",
      "Processing file Bukowski_1946_66.txt\n",
      "Processed a total of 68615 lines.\n"
     ]
    }
   ],
   "source": [
    "size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "start_symbol = 'START_SYMBOL'\n",
    "stop_symbol = 'STOP_SYMBOL'\n",
    "\n",
    "## Collect input files.\n",
    "files = []\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        files.append(file)\n",
    "\n",
    "## Create empty data frame.\n",
    "content = pd.DataFrame({'line' : [], 'text' : [], 'tokens' : [], 'poem' : []})\n",
    "\n",
    "## Fill dataframe from files.\n",
    "for file in files:\n",
    "    print \"Processing file\", file\n",
    "    for l in open(file, 'rb'):\n",
    "        line = l.strip('\\n')\n",
    "        if l[0].isdigit():\n",
    "            data = line.split('   ')\n",
    "            if len(data) > 1:\n",
    "                #sentence = start_symbol + ' ' + data[1] + ' ' + stop_symbol\n",
    "                sentence = data[1]\n",
    "                tokens = nltk.word_tokenize(sentence.decode('utf-8').lower())\n",
    "                ## Add start and stop symbols.\n",
    "                tokens.insert(0, start_symbol)\n",
    "                tokens.append(stop_symbol)\n",
    "                content = content.append({'line' : int(data[0]), 'text' : data[1], 'tokens' : tokens}, ignore_index = True)\n",
    "\n",
    "## Make the 'line' content numeric \n",
    "content['line'] = pd.to_numeric(content['line'])\n",
    "print \"Processed a total of %d lines.\" %content.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1363 distinct poems.\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "poem = 1\n",
    "for i, row in content.iterrows():\n",
    "    if row['line'] >= count:\n",
    "        count = row['line']\n",
    "        content.loc[i, 'poem'] = poem\n",
    "    else:\n",
    "        count = 1\n",
    "        poem += 1\n",
    "        content.loc[i, 'poem'] = poem\n",
    "## Store results on csv.        \n",
    "#content.to_csv('content.csv', index = False)\n",
    "print \"Parsed %d distinct poems.\" %poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16271 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "## Get word frequency.\n",
    "sentences = [content.loc[i, 'tokens'] for i in content.index]\n",
    "flattened = itertools.chain.from_iterable(sentences) \n",
    "word_freq = nltk.FreqDist(flattened)\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'improve' and appeared 2 times.\n"
     ]
    }
   ],
   "source": [
    "## Limit vocabulary to most common words.\n",
    "vocab = word_freq.most_common(size-1)\n",
    "word_list = [x[0] for x in vocab]\n",
    "word_list.append(unknown_token)\n",
    "word_list\n",
    "word_index = dict([(w,i) for i,w in enumerate(word_list)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed poem:\n",
      "START_SYMBOL nothing matters STOP_SYMBOL\n",
      "START_SYMBOL but flopping on a mattress STOP_SYMBOL\n",
      "START_SYMBOL with cheap dreams and a beer STOP_SYMBOL\n",
      "START_SYMBOL as the leaves die and the horses die STOP_SYMBOL\n",
      "START_SYMBOL and the landladies stare in the halls ; STOP_SYMBOL\n",
      "START_SYMBOL brisk the music of pulled shades , STOP_SYMBOL\n",
      "START_SYMBOL a last man 's cave STOP_SYMBOL\n",
      "START_SYMBOL in an UNKNOWN_TOKEN of swarm STOP_SYMBOL\n",
      "START_SYMBOL and explosion ; STOP_SYMBOL\n",
      "START_SYMBOL nothing but the dripping sink , STOP_SYMBOL\n",
      "START_SYMBOL the empty bottle , STOP_SYMBOL\n",
      "START_SYMBOL UNKNOWN_TOKEN , STOP_SYMBOL\n",
      "START_SYMBOL youth UNKNOWN_TOKEN in , STOP_SYMBOL\n",
      "START_SYMBOL stabbed and shaven , STOP_SYMBOL\n",
      "START_SYMBOL taught words STOP_SYMBOL\n",
      "START_SYMBOL propped up STOP_SYMBOL\n",
      "START_SYMBOL to die . STOP_SYMBOL\n"
     ]
    }
   ],
   "source": [
    "## Replace words not in dictionary with the unknown token.\n",
    "#for i, row in content.iterrows():\n",
    "#    for j, tkn in enumerate(row['tokens']):\n",
    "#        if tkn not in word_list:\n",
    "#            content.loc[i, 'tokens'][j] = unknown_token\n",
    "            \n",
    "print 'Sample processed poem:'\n",
    "for index, row in content[content['poem'] == 10].iterrows():\n",
    "    print ' '.join(row['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a generative model, the output we get is the word most likely to follow given an input sequence. Our $x$ representation for each sentence is a vector of the first $n-1$ words (numerically encoded), while $y$ is another $n-1$ vector where $y_1$ word following each $x_i$. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Store data in pickle for later use.\n",
    "content.to_pickle('data/content.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input data:  START_SYMBOL there wo n't be any more , abstract or otherwise ;\n",
      "Example Output data: there wo n't be any more , abstract or otherwise ; STOP_SYMBOL\n"
     ]
    }
   ],
   "source": [
    "## Convert the info into training data.\n",
    "X_train = np.asarray([[word_index[w] for w in sent[:-1]] for sent in content['tokens'].tolist()])\n",
    "y_train = np.asarray([[word_index[w] for w in sent[1:]] for sent in content['tokens'].tolist()])\n",
    "\n",
    "## Show an example.\n",
    "x_example, y_example = X_train[39], y_train[39]\n",
    "print 'Example Input data: ', \n",
    "input_sent = [word_list[i] for i in x_example]\n",
    "print ' '.join(x for x in input_sent)\n",
    "\n",
    "print 'Example Output data:', \n",
    "output_sent = [word_list[j] for j in y_example]\n",
    "print ' '.join(x for x in output_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "Now that we have the data in the desired format, we start building the neural network. We create a **RNNNumpy** class that contains the following functions:\n",
    "\n",
    "* *__init__*  → Initializes parameters based on the number of word dimensions, hidden dimensions and the uniform distribution $ \\left[\\dfrac{-1}{\\sqrt{n}}, \\dfrac{1}{\\sqrt{n}}\\right] $ for (U, V & W).\n",
    "\n",
    "* *forward_propagation* → Generates the word probabilities by unfolding the neural network. Uses $tanh$ activation function for the input layer and $softmax$ for the output one.\n",
    "\n",
    "* *predict* → Returns estimated output (i.e. the word with the highest probability of occuring next) from the forward propagation estimates.\n",
    "\n",
    "* *calculate_loss* → Cross-entropy loss from prediction, as defined by the function below.\n",
    "\n",
    "    $ L(y,o) = \\dfrac{-1}{N} \\sum y_n log(o_n)$\n",
    "\n",
    "  \n",
    "* *bptt* → Back-propagation algorithm, used to estimate gradients and update parameters.\n",
    "\n",
    "* *gradient_check* → Safety measure for validating that our BPPT estimates are going in the right direction.\n",
    "\n",
    "    $ \\dfrac{\\partial L}{\\partial \\theta} \\approx J(\\theta + h) - \\dfrac{J(\\theta + h)}{2 h} $\n",
    "\n",
    "  \n",
    "* *sgd_step* → SGD to calculate gradients and perform updates in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define our softmax function.\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self, w_dim, h_dim=100, bptt_max=4):\n",
    "        self.w_dim = w_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.bptt_max = bptt_max\n",
    "        ## Randomly initialize network parameters.\n",
    "        ## Set to uniform between [-1/n, 1/n], where\n",
    "        ## 'n' is the size of incoming connections.\n",
    "        self.U = np.random.uniform(-np.sqrt(1./w_dim), np.sqrt(1./w_dim), (h_dim, w_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (w_dim, h_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (h_dim, h_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        ## Length of inputs.\n",
    "        T = len(x)\n",
    "        ## Generate matrix for all hidden states and all outputs.\n",
    "        s = np.zeros((T+1, self.h_dim))\n",
    "        o = np.zeros((T, self.w_dim))\n",
    "        ## Iterate though the steps.\n",
    "        for t in np.arange(T):\n",
    "            # s[t] = Ux[t] = Ws[t-1]\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            # o[t] = softmax(Vs[t])\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def total_loss_function(self, x, y):\n",
    "        L = 0\n",
    "        ## Calculate loss on each sentence.\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            ## Select predictions for correct words.\n",
    "            correct_preds = o[np.arange(len(y[i])), y[i]]\n",
    "            L -= np.sum(np.log(correct_preds))\n",
    "        return L\n",
    "    \n",
    "    def loss_function(self, x, y):\n",
    "        N = np.sum(len(y_i) for y_i in y)\n",
    "        return self.total_loss_function(x,y)/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        ## Start with forward-propagation.\n",
    "        o, s = self.forward_propagation(x)\n",
    "        dLdU = np.zeros_like(self.U)\n",
    "        dLdV = np.zeros_like(self.V)\n",
    "        dLdW = np.zeros_like(self.W)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1\n",
    "        ## Go over observations, from end to start.\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV = np.outer(delta_o[t], s[t].T)\n",
    "            ## Initial delta.\n",
    "            delta_t = np.inner(self.V.T, delta_o[t]) * (1 - s[t]**2)\n",
    "            ## Now we do the back-propagation.\n",
    "            for step in np.arange(max(0, t - self.bptt_max), t+1)[::-1]:\n",
    "                dLdW = np.outer(delta_t, s[step - 1]) \n",
    "                dLdU[:, x[step]] += delta_t\n",
    "                ## Update delta for next iteration.\n",
    "                delta_t = np.inner(self.W.T, delta_t) * (1 - s[step - 1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        ## Calculate the parameters with bptt.\n",
    "        bptt_gradients = model.bptt(x,y)\n",
    "        ## Parameters to check.\n",
    "        model_params = ['U','V','W']\n",
    "        ## Perform 'manual' check on each parameter.\n",
    "        for pid, pname in enumerate(model_params):\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print 'Performing gradient check on %s with size %d.' %(pname, np.prod(parameter.shape))\n",
    "            ## Iterate over elements of the parameter matrix.\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                ## Store original value.\n",
    "                original_val = parameter[ix]\n",
    "                ## Estimate gradient manually.\n",
    "                parameter[ix] = original_val + h\n",
    "                grad_plus = model.total_loss_function([x], [y])\n",
    "                parameter[ix] = original_val - h\n",
    "                grad_minus = model.total_loss_function([x], [y])\n",
    "                estimated_gradient = (grad_plus - grad_minus) / (2*h)\n",
    "                ## Reset parameter to original value.\n",
    "                parameter[ix] = original_val\n",
    "                ## Estimate gradient with bptt.\n",
    "                backprop_gradient = bptt_gradients[pid][ix]\n",
    "                ## Calculate relative error.\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient) / \\\n",
    "                (np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                ## If the error is to large, do not pass the test.\n",
    "                if relative_error > error_threshold:\n",
    "                    print 'Gradient check ERROR: parameter=%s, index=%s.' %(pname, ix)\n",
    "                    print 'Relative Error: %f.' %relative_error\n",
    "                it.iternext()\n",
    "            print 'Gradient check for parameter %s passed! :)' %pname\n",
    "            \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        ## Calculate gradients with bptt.\n",
    "        dLdU, dLdV, dLdW = self.bptt(x,y)\n",
    "        ## Update according to gradient and learning rate.\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, epochs=100, evaluate_loss_after=5):\n",
    "    ## List to keep track of losses.\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            loss = model.loss_function(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%H:%M:%S')\n",
    "            print '%s: Loss after num_examples=%d & epoch=%d: %f.' %(time, num_examples_seen, epoch, loss)\n",
    "            \n",
    "            ## Adjust learning rate if loss increases.\n",
    "            if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print 'Setting learning rate to %f.' %learning_rate\n",
    "            \n",
    "            ## For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            ## do one SGD step.\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "\n",
    "At this point the algorithm is complete. Now its a good time to perform some sanity checks and get an idea of how our algorithm is working. Below:\n",
    "\n",
    "* We look at the running time for one SGD step (`train_with_sgd`).\n",
    "\n",
    "* Ve verify that indeed our BPTT gradient descend reducing the error. How does this compare to a random prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 47.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "## Running time of one SGD.\n",
    "model = RNNNumpy(size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected random prediction loss: 8.987197.\n",
      "14:53:20: Loss after num_examples=0 & epoch=0: 8.986688.\n",
      "16:46:33: Loss after num_examples=68615 & epoch=1: 7.390060.\n",
      "19:25:04: Loss after num_examples=137230 & epoch=2: 7.250461.\n",
      "20:29:17: Loss after num_examples=205845 & epoch=3: 7.189111.\n",
      "21:24:06: Loss after num_examples=274460 & epoch=4: 7.150902.\n",
      "22:24:48: Loss after num_examples=343075 & epoch=5: 7.122543.\n",
      "23:43:07: Loss after num_examples=411690 & epoch=6: 7.099471.\n",
      "00:37:56: Loss after num_examples=480305 & epoch=7: 7.080854.\n",
      "01:30:33: Loss after num_examples=548920 & epoch=8: 7.065424.\n",
      "02:23:17: Loss after num_examples=617535 & epoch=9: 7.051475.\n"
     ]
    }
   ],
   "source": [
    "print \"Expected random prediction loss: %f.\" % np.log(size)\n",
    "\n",
    "## Verify that indeed the BPTT algorithm reduces the error over time.\n",
    "losses = train_with_sgd(model, X_train, y_train, epochs=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7ff6b64981d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAHrCAYAAAAJ7AMUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOoXiGYgAAAANSURBVIhABQAA0EH/H8eTczgjYELBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6b6e671d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([l[1] for l in losses], 'go')\n",
    "plt.plot([l[1] for l in losses], 'g-')\n",
    "plt.axhline(np.log(size))\n",
    "plt.title('Random Prediction vs. Model Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without optimizing our code any further, the total running time of the model is about ??? days (quite long). Using theano should decrease the running time by a large factor. On the other hand the results from the `train_with_sgd` algorithm are encouraging, as the error is indeed decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sentences\n",
    "We can use this model in a generative fashion to create new sentences that have the highest probability of looking like Bukowski's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the\n",
      "the the the\n",
      "the the the\n",
      "the the the\n",
      "the the the the\n",
      "the the the the\n",
      "the the the the the the\n",
      "the the the the\n",
      "the the the\n",
      "the the the\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_index[start_symbol]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_index[stop_symbol]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1][0])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [word_list[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 3\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
