{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano Bukowski\n",
    "On this notebook I improve the [Numpy Bukowski](https://github.com/masta-g3/summer_networks/tree/master/numpy_bukowski) poetry model by porting the code to Theano, which should perform much faster. The preprocessing section remains exactly the same, so it might be good to load the preprocessed data from disk instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import libaries.\n",
    "import itertools\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "First step is to read the poem files, store them in a convenient location (pandas DF) and keep some labeled information that might be useful in the future. The words on the lines of the poems will be tokenized, and each word will receive an index value more easily consumable by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file Bukowski_1955_73.txt\n",
      "Processing file Bukowski_1960_95.txt\n",
      "Processing file Bukowski_1972.txt\n",
      "Processing file Bukowski_1974_77.txt\n",
      "Processing file Bukowski_1960_97.txt\n",
      "Processing file Bukowski_1960_99.txt\n",
      "Processing file Bukowski_1960_97(2).txt\n",
      "Processing file Bukowski_1981_84.txt\n",
      "Processing file Bukowski_1986.txt\n",
      "Processing file Bukowski_1960_96.txt\n",
      "Processing file Bukowski_1946_66.txt\n",
      "Processed a total of 68615 lines.\n"
     ]
    }
   ],
   "source": [
    "size = 10000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "start_symbol = 'START_SYMBOL'\n",
    "stop_symbol = 'STOP_SYMBOL'\n",
    "\n",
    "## Collect input files.\n",
    "files = []\n",
    "for file in os.listdir('.'):\n",
    "    if file.endswith('.txt'):\n",
    "        files.append(file)\n",
    "\n",
    "## Create empty data frame.\n",
    "content = pd.DataFrame({'line' : [], 'text' : [], 'tokens' : [], 'poem' : []})\n",
    "\n",
    "## Fill dataframe from files.\n",
    "for file in files:\n",
    "    print 'Processing file', file\n",
    "    for l in open(file, 'rb'):\n",
    "        line = l.strip('\\n')\n",
    "        if l[0].isdigit():\n",
    "            data = line.split('   ')\n",
    "            if len(data) > 1:\n",
    "                #sentence = start_symbol + ' ' + data[1] + ' ' + stop_symbol\n",
    "                sentence = data[1]\n",
    "                tokens = nltk.word_tokenize(sentence.decode('utf-8').lower())\n",
    "                ## Add start and stop symbols.\n",
    "                tokens.insert(0, start_symbol)\n",
    "                tokens.append(stop_symbol)\n",
    "                content = content.append({'line' : int(data[0]), 'text' : data[1], 'tokens' : tokens}, ignore_index = True)\n",
    "\n",
    "## Make the 'line' content numeric \n",
    "content['line'] = pd.to_numeric(content['line'])\n",
    "print 'Processed a total of %d lines.' %content.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1363 distinct poems.\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "poem = 1\n",
    "for i, row in content.iterrows():\n",
    "    if row['line'] >= count:\n",
    "        count = row['line']\n",
    "        content.loc[i, 'poem'] = poem\n",
    "    else:\n",
    "        count = 1\n",
    "        poem += 1\n",
    "        content.loc[i, 'poem'] = poem\n",
    "## Store results on csv.        \n",
    "#content.to_csv('content.csv', index = False)\n",
    "print 'Parsed %d distinct poems.' %poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16271 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "## Get word frequency.\n",
    "sentences = [content.loc[i, 'tokens'] for i in content.index]\n",
    "flattened = itertools.chain.from_iterable(sentences) \n",
    "word_freq = nltk.FreqDist(flattened)\n",
    "print 'Found %d unique words tokens.' % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'pandered' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "## Limit vocabulary to most common words.\n",
    "vocab = word_freq.most_common(size-1)\n",
    "word_list = [x[0] for x in vocab]\n",
    "word_list.append(unknown_token)\n",
    "word_list\n",
    "word_index = dict([(w,i) for i,w in enumerate(word_list)])\n",
    "\n",
    "print 'Using vocabulary size %d.' % size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed poem:\n",
      "START_SYMBOL i awakened to dryness and the ferns were dead , STOP_SYMBOL\n",
      "START_SYMBOL the potted plants yellow as corn ; STOP_SYMBOL\n",
      "START_SYMBOL my woman was gone STOP_SYMBOL\n",
      "START_SYMBOL and the empty bottles like bled corpses STOP_SYMBOL\n",
      "START_SYMBOL surrounded me with their uselessness ; STOP_SYMBOL\n",
      "START_SYMBOL the sun was still good , though , STOP_SYMBOL\n",
      "START_SYMBOL and my landlady 's note cracked in fine and STOP_SYMBOL\n",
      "START_SYMBOL undemanding UNKNOWN_TOKEN ; what was needed now STOP_SYMBOL\n",
      "START_SYMBOL was a good comedian , ancient style , a UNKNOWN_TOKEN STOP_SYMBOL\n",
      "START_SYMBOL with jokes upon absurd pain ; pain is absurd STOP_SYMBOL\n",
      "START_SYMBOL because it exists , nothing more ; STOP_SYMBOL\n",
      "START_SYMBOL i shaved carefully with an old razor STOP_SYMBOL\n",
      "START_SYMBOL the man who had once been young and STOP_SYMBOL\n",
      "START_SYMBOL said to have genius ; but STOP_SYMBOL\n",
      "START_SYMBOL that 's the tragedy of the leaves , STOP_SYMBOL\n",
      "START_SYMBOL the dead ferns , the dead plants ; STOP_SYMBOL\n",
      "START_SYMBOL and i walked into a dark hall STOP_SYMBOL\n",
      "START_SYMBOL where the landlady stood STOP_SYMBOL\n",
      "START_SYMBOL UNKNOWN_TOKEN and final , STOP_SYMBOL\n",
      "START_SYMBOL sending me to hell , STOP_SYMBOL\n",
      "START_SYMBOL waving her fat , sweaty arms STOP_SYMBOL\n",
      "START_SYMBOL and screaming STOP_SYMBOL\n",
      "START_SYMBOL screaming for rent STOP_SYMBOL\n",
      "START_SYMBOL because the world had failed us STOP_SYMBOL\n",
      "START_SYMBOL both . STOP_SYMBOL\n"
     ]
    }
   ],
   "source": [
    "## Replace words not in dictionary with the unknown token.\n",
    "for i, row in content.iterrows():\n",
    "    for j, tkn in enumerate(row['tokens']):\n",
    "        if tkn not in word_list:\n",
    "            content.loc[i, 'tokens'][j] = unknown_token\n",
    "            \n",
    "print 'Sample processed poem:'\n",
    "for index, row in content[content['poem'] == 1].iterrows():\n",
    "    print ' '.join(row['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input data:  START_SYMBOL there is a UNKNOWN_TOKEN of some sort\n",
      "Example Output data: there is a UNKNOWN_TOKEN of some sort STOP_SYMBOL\n"
     ]
    }
   ],
   "source": [
    "## Convert the info into training data.\n",
    "X_train = np.asarray([[word_index[w] for w in sent[:-1]] for sent in content['tokens'].tolist()])\n",
    "y_train = np.asarray([[word_index[w] for w in sent[1:]] for sent in content['tokens'].tolist()])\n",
    "\n",
    "## Show an example.\n",
    "x_example, y_example = X_train[49], y_train[49]\n",
    "print 'Example Input data: ', \n",
    "input_sent = [word_list[i] for i in x_example]\n",
    "print ' '.join(x for x in input_sent)\n",
    "\n",
    "print 'Example Output data:', \n",
    "output_sent = [word_list[j] for j in y_example]\n",
    "print ' '.join(x for x in output_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "Now that we have the data in the desired format, we start building the neural network. Our **RNNNumpy** class is made up by the following functions:\n",
    "\n",
    "* **__init__**: Initializes parameters based on the number of word dimensions, hidden dimensions and the uniform distribution $ \\left[\\dfrac{-1}{\\sqrt{n}}, \\dfrac{1}{\\sqrt{n}}\\right] $ for (U, V & W).\n",
    "\n",
    "* **forward_propagation**: Generates the word probabilities by unfolding the neural network. Uses *tanh* activation function for the input layer and *softmax* for the output one.\n",
    "\n",
    "* **predict**: Returns estimated output (i.e. the word with the highest probability of occuring next) from the forward propagation estimates.\n",
    "\n",
    "* **calculate_loss**: Cross-entropy loss from prediction, as defined by the function below.\n",
    "\n",
    "    $ L(y,o) = \\dfrac{-1}{N} \\sum y_n log(o_n)$\n",
    "\n",
    "  \n",
    "* **bptt**: Back-propagation algorithm, used to estimate gradients and update parameters.\n",
    "\n",
    "* **gradient_check**: Safety measure for validating that our BPPT estimates are going in the right direction.\n",
    "\n",
    "    $ \\dfrac{\\partial L}{\\partial \\theta} \\approx J(\\theta + h) - \\dfrac{J(\\theta + h)}{2 h} $\n",
    "\n",
    "  \n",
    "* **sgd_step**: SGD to calculate gradients and perform updates in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNTheano:\n",
    "    \n",
    "    def __init__(self, w_dim, h_dim=100, bptt_max=4):\n",
    "        \n",
    "        ## Set self parameters.\n",
    "        self.w_dim = w_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.bptt_max = bptt_max    \n",
    "        \n",
    "        ## Randomly initialize network parameters. Set to uniform between\n",
    "        ## [-1/n, 1/n], where 'n' is the size of incoming connections.\n",
    "        U = np.random.uniform(-np.sqrt(1./w_dim), np.sqrt(1./w_dim), (h_dim, w_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (w_dim, h_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (h_dim, h_dim))\n",
    "        \n",
    "        ## Assign to self as theano shared variables.\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='U', value=V.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        \n",
    "        ## Build Model.\n",
    "        self.theano = {}\n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        ## Define parameters.\n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        \n",
    "        ## Forward propagation to perform at each step.\n",
    "        def fwd_step(x_t, s_t_prev, U, V, W):\n",
    "            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            o_t = T.nnet.softmax(V.dot(s_t))\n",
    "            ## Softmax returns a matrix, so we convert it into a vector.\n",
    "            return [o_t[0], s_t]\n",
    "        \n",
    "        ## Iterate over all observations\n",
    "        [o,s], updates = theano.scan(fwd_step,\n",
    "                                     sequences=x,\n",
    "                                     outputs_info=[None, dict(initial = T.zeros(self.h_dim))],\n",
    "                                     non_sequences=[U,V,W],\n",
    "                                     truncate_gradient=self.bptt_max,\n",
    "                                     strict=True)\n",
    "        \n",
    "        ## Prediction and loss.\n",
    "        predict = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        ## Define gradients.\n",
    "        dU = T.grad(o_error, U)\n",
    "        dV = T.grad(o_error, V)\n",
    "        dW = T.grad(o_error, W)\n",
    "        \n",
    "        ## Convert graph into actual functions.\n",
    "        self.fwd_prop = theano.function([x], o)\n",
    "        self.predict = theano.function([x], predict)\n",
    "        self.ce_error = theano.function([x,y], o_error)\n",
    "        self.bptt = theano.function([x,y], [dU, dV, dW])\n",
    "    \n",
    "        ## SGD.\n",
    "        l_rate = T.scalar('l_rate')\n",
    "        self.sgd_step = theano.function([x,y,l_rate], [],\n",
    "                                        updates = [(U, U - l_rate * dU),\n",
    "                                                   (V, V - l_rate * dV),\n",
    "                                                   (W, W - l_rate * dW)])\n",
    "    \n",
    "    ## Total loss function...\n",
    "    def total_loss_function(self, X, Y):\n",
    "        \n",
    "        return np.sum([self.ce_error(x, y) for x,y in zip(X,Y)])\n",
    "        \n",
    "    ## Loss by words.\n",
    "    def loss_function(self, X, Y):\n",
    "        n = np.sum([len(y) for y in Y])\n",
    "        return self.total_loss_function(X, Y) / float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, l_rate=0.005, epochs=100, evaluate_loss_after=5):\n",
    "    ## List to keep track of losses.\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            loss = model.loss_function(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%H:%M:%S')\n",
    "            print '%s: Loss after num_examples=%d & epoch=%d: %f.' %(time, num_examples_seen, epoch, loss)\n",
    "            \n",
    "            ## Adjust learning rate if loss increases.\n",
    "            if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                l_rate = l_rate * 0.5\n",
    "                print 'Setting learning rate to %f.' %l_rate\n",
    "            \n",
    "        ## Save model parameters\n",
    "        #save_model_parameters_theano('./data/rnn-theano-%d-%d-%s.npz' % (model.h_din, model.w_dim, time), model)\n",
    "        \n",
    "        ## For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            ## do one SGD step.\n",
    "            model.sgd_step(X_train[i], y_train[i], l_rate)\n",
    "            num_examples_seen += 1\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "\n",
    "At this point the algorithm is complete. Now its a good time to perform some sanity checks and get an idea of how our algorithm is working. Below:\n",
    "\n",
    "* We look at the running time for one SGD step (`train_with_sgd`).\n",
    "\n",
    "* Ve verify that indeed our BPTT gradient descend reducing the error. How does this compare to a random prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create model with theano.\n",
    "model = RNNTheano(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected random prediction loss: 9.210340.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-c53ac56b0a91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m## Verify that indeed the algorithm reduces the error over time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_with_sgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_loss_after\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-4b1055dfa43a>\u001b[0m in \u001b[0;36mtrain_with_sgd\u001b[1;34m(model, X_train, y_train, l_rate, epochs, evaluate_loss_after)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_loss_after\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_examples_seen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-9384eb6fe904>\u001b[0m in \u001b[0;36mloss_function\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-9384eb6fe904>\u001b[0m in \u001b[0;36mtotal_loss_function\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtotal_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mce_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m## Loss by words.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/manuel/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/scan_perform/mod.cpp:6489)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mvalue_zeros\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m         \"\"\"\n\u001b[0;32m    631\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[1;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print 'Expected random prediction loss: %f.' % np.log(size)\n",
    "\n",
    "## Verify that indeed the algorithm reduces the error over time.\n",
    "losses = train_with_sgd(model, X_train, y_train, epochs=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([l[1] for l in losses], 'go')\n",
    "plt.plot([l[1] for l in losses], 'g-')\n",
    "plt.axhline(np.log(size))\n",
    "plt.title('Random Prediction vs. Model Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without optimizing our code any further, the total running time of the model is about ??? days (quite long). Using theano should decrease the running time by a large factor. On the other hand the results from the `train_with_sgd` algorithm are encouraging, as the error is indeed decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sentences\n",
    "We can use this model in a generative fashion to create new sentences that have the highest probability of looking like Bukowski's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: A.shape[1] != x.shape[0]\nApply node that caused the error: CGemv{no_inplace}(Subtensor{::, int32}.0, TensorConstant{1.0}, W_copy, <TensorType(float64, vector)>, TensorConstant{1.0})\nToposort index: 4\nInputs types: [TensorType(float64, vector), TensorType(float64, scalar), TensorType(float64, matrix), TensorType(float64, vector), TensorType(float64, scalar)]\nInputs shapes: [(100,), (), (100, 10000), (100,), ()]\nInputs strides: [(80000,), (), (80000, 8), (8,), ()]\nInputs values: ['not shown', array(1.0), 'not shown', 'not shown', array(1.0)]\nOutputs clients: [[Elemwise{tanh,no_inplace}(CGemv{no_inplace}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{0}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0, U, U, W)\nToposort index: 7\nInputs types: [TensorType(int64, scalar), TensorType(int32, vector), TensorType(float64, matrix), TensorType(int64, scalar), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix)]\nInputs shapes: [(), (1,), (1, 100), (), (100, 10000), (100, 10000), (100, 10000)]\nInputs strides: [(), (4,), (800, 8), (), (80000, 8), (80000, 8), (80000, 8)]\nInputs values: [array(1), array([0], dtype=int32), 'not shown', array(1), 'not shown', 'not shown', 'not shown']\nOutputs clients: [[], ['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bd6998bb3611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# We want long sentences, not sentences with one or two words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msenten_min_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-bd6998bb3611>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Repeat until we get an end token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnew_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstop_symbol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mnext_word_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfwd_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0msampled_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munknown_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# We don't want to sample unknown words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/manuel/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/scan_perform/mod.cpp:4316)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/manuel/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/scan_perform/mod.cpp:4193)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape mismatch: A.shape[1] != x.shape[0]\nApply node that caused the error: CGemv{no_inplace}(Subtensor{::, int32}.0, TensorConstant{1.0}, W_copy, <TensorType(float64, vector)>, TensorConstant{1.0})\nToposort index: 4\nInputs types: [TensorType(float64, vector), TensorType(float64, scalar), TensorType(float64, matrix), TensorType(float64, vector), TensorType(float64, scalar)]\nInputs shapes: [(100,), (), (100, 10000), (100,), ()]\nInputs strides: [(80000,), (), (80000, 8), (8,), ()]\nInputs values: ['not shown', array(1.0), 'not shown', 'not shown', array(1.0)]\nOutputs clients: [[Elemwise{tanh,no_inplace}(CGemv{no_inplace}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{0}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0, U, U, W)\nToposort index: 7\nInputs types: [TensorType(int64, scalar), TensorType(int32, vector), TensorType(float64, matrix), TensorType(int64, scalar), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix)]\nInputs shapes: [(), (1,), (1, 100), (), (100, 10000), (100, 10000), (100, 10000)]\nInputs strides: [(), (4,), (800, 8), (), (80000, 8), (80000, 8), (80000, 8)]\nInputs values: [array(1), array([0], dtype=int32), 'not shown', array(1), 'not shown', 'not shown', 'not shown']\nOutputs clients: [[], ['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_index[start_symbol]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_index[stop_symbol]:\n",
    "        next_word_probs = model.fwd_prop(new_sentence)\n",
    "        sampled_word = word_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [word_list[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 4\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
