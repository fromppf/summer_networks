{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bukowski 2.0\n",
    "\n",
    "On this experiment we will try to generate a deep neural network (a LSTM RNN in the end) to build an algorithm that can write poems like Bukowski did. For this we will take all his available poems and tokenize them to use them as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We start by reading the files, storing them in a convenient location and keep some labeled information that might be useful in the future. We also tokenize each of the lines in the poems, and give each word an index value that can be consumed by our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file Bukowski_1974_77.txt\n",
      "Processing file Bukowski_1960_97.txt\n",
      "Processed a total of 15576 lines.\n"
     ]
    }
   ],
   "source": [
    "size = 1000\n",
    "unknown_token = 'unknown_token'\n",
    "start_symbol = 'start_symbol'\n",
    "stop_symbol = 'stop_symbol'\n",
    "\n",
    "## Collect input files.\n",
    "files = []\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\"7.txt\"):\n",
    "        files.append(file)\n",
    "\n",
    "## Create empty data frame.\n",
    "content = pd.DataFrame({'line' : [], 'text' : [], 'tokens' : [], 'poem' : []})\n",
    "\n",
    "## Fill dataframe from files.\n",
    "for file in files:\n",
    "    print \"Processing file\", file\n",
    "    for l in open(file, 'rb'):\n",
    "        line = l.strip('\\n')\n",
    "        if l[0].isdigit():\n",
    "            data = line.split('   ')\n",
    "            if len(data) > 1:\n",
    "                sentence = start_symbol + ' ' + data[1] + ' ' + stop_symbol\n",
    "                tokens = nltk.word_tokenize(sentence.decode('utf-8').lower())\n",
    "                content = content.append({'line' : int(data[0]), 'text' : data[1], 'tokens' : tokens}, ignore_index = True)\n",
    "\n",
    "## Make the 'line' content numeric \n",
    "content['line'] = pd.to_numeric(content['line'])\n",
    "print \"Processed a total of %d lines.\" %content.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 334 distinct poems.\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "poem = 1\n",
    "for i, row in content.iterrows():\n",
    "    if row['line'] >= count:\n",
    "        count = row['line']\n",
    "        content.loc[i, 'poem'] = poem\n",
    "    else:\n",
    "        count = 1\n",
    "        poem += 1\n",
    "        content.loc[i, 'poem'] = poem\n",
    "## Store results on csv.        \n",
    "content.to_csv('content.csv', index = False)\n",
    "print \"Parsed %d distinct poems.\" %poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6515 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "## Get word frequency.\n",
    "sentences = [content.loc[i, 'tokens'] for i in content.index]\n",
    "flattened = itertools.chain.from_iterable(sentences) \n",
    "word_freq = nltk.FreqDist(flattened)\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1000.\n",
      "The least frequent word in our vocabulary is 'victory' and appeared 7 times.\n"
     ]
    }
   ],
   "source": [
    "## Limit vocabulary to most common words.\n",
    "vocab = word_freq.most_common(size-1)\n",
    "word_list = [x[0] for x in vocab]\n",
    "word_list.append(unknown_token)\n",
    "word_list\n",
    "word_index = dict([(w,i) for i,w in enumerate(word_list)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Replace words not in dictionary with the unknown token.\n",
    "for i, row in content.iterrows():\n",
    "    for j, tkn in enumerate(row['tokens']):\n",
    "        if tkn not in word_list:\n",
    "            content.loc[i, 'tokens'][j] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  [u'start_symbol', 'unknown_token', 'unknown_token', u'out', u'of']\n",
      "Output data: ['unknown_token', 'unknown_token', u'out', u'of', u'stop_symbol']\n"
     ]
    }
   ],
   "source": [
    "## Convert the info into training data.\n",
    "X_train = np.asarray([[word_index[w] for w in sent[:-1]] for sent in content['tokens'].tolist()])\n",
    "y_train = np.asarray([[word_index[w] for w in sent[1:]] for sent in content['tokens'].tolist()])\n",
    "\n",
    "## Show an example.\n",
    "x_example, y_example = X_train[10], y_train[10]\n",
    "print 'Input data: ', [word_list[i] for i in x_example]\n",
    "print 'Output data:', [word_list[j] for j in y_example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network\n",
    "Now that we have the data in the desired format, we start building the neural network. Our **RNNNumpy** class is made up by the following functions:\n",
    "\n",
    "**__init__**: Initializes parameters based on the number of word dimensions, hidden dimensions and the uniform distribution $ \\left[\\dfrac{-1}{\\sqrt{n}}, \\dfrac{1}{\\sqrt{n}}\\right] $ for (U, V & W).\n",
    "\n",
    "**forward_propagation**: What the name implies, using a *tanh* activation function for the input layer and *softmax* for the output one.\n",
    "\n",
    "**predict**: Returns output with highest probability from the forward propagation estimates.\n",
    "\n",
    "**calculate_loss**: Cross entropy loss function, as defined below.\n",
    "\n",
    "$ L(y,o) = \\dfrac{-1}{N} \\sum y_n log(o_n)$\n",
    "\n",
    "**bptt**: Back propagation algorithm, used to estimate parameters.\n",
    "\n",
    "**gradient_check**: Safety measure for validating that our BPPT estimates are going in the right direction.\n",
    "\n",
    "$ \\dfrac{\\partial L}{\\partial \\theta} \\approx J(\\theta + h) - \\dfrac{J(\\theta + h)}{2 h} $\n",
    "\n",
    "**sgd_step**: SGD to calculate gradients and perform updates in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define our softmax function.\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self, w_dim, h_dim=100, bptt_max=4):\n",
    "        self.w_dim = w_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.bptt_max = bptt_max\n",
    "        ## Randomly initialize network parameters.\n",
    "        ## Set to uniform between [-1/n, 1/n], where\n",
    "        ## 'n' is the size of incoming connections.\n",
    "        self.U = np.random.uniform(-np.sqrt(1./w_dim), np.sqrt(1./w_dim), (h_dim, w_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (w_dim, h_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (h_dim, h_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        ## Length of inputs.\n",
    "        T = len(x)\n",
    "        ## Generate matrix for all hidden states and all outputs.\n",
    "        s = np.zeros((T+1, self.h_dim))\n",
    "        o = np.zeros((T, self.w_dim))\n",
    "        ## Iterate though the steps.\n",
    "        for t in np.arange(T):\n",
    "            # s[t] = Ux[t] = Ws[t-1]\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            # o[t] = softmax(Vs[t])\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def total_loss_function(self, x, y):\n",
    "        L = 0\n",
    "        ## Calculate loss on each sentence.\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            ## Select predictions for correct words.\n",
    "            correct_preds = o[np.arange(len(y[i])), y[i]]\n",
    "            L -= np.sum(np.log(correct_preds))\n",
    "        return L\n",
    "    \n",
    "    def loss_function(self, x, y):\n",
    "        N = np.sum(len(y_i) for y_i in y)\n",
    "        return self.total_loss_function(x,y)/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        ## Start with forward-propagation.\n",
    "        o, s = self.forward_propagation(x)\n",
    "        dLdU = np.zeros_like(self.U)\n",
    "        dLdV = np.zeros_like(self.V)\n",
    "        dLdW = np.zeros_like(self.W)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1\n",
    "        ## Go over observations, from end to start.\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV = np.outer(delta_o[t], s[t].T)\n",
    "            ## Initial delta.\n",
    "            delta_t = np.inner(self.V.T, delta_o[t]) * (1 - s[t]**2)\n",
    "            ## Now we do the back-propagation.\n",
    "            for step in np.arange(max(0, t - self.bptt_max), t+1)[::-1]:\n",
    "                dLdW = np.outer(delta_t, s[step - 1]) \n",
    "                dLdU[:, x[step]] += delta_t\n",
    "                ## Update delta for next iteration.\n",
    "                delta_t = np.inner(self.W.T, delta_t) * (1 - s[step - 1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        ## Calculate the parameters with bptt.\n",
    "        bptt_gradients = model.bptt(x,y)\n",
    "        ## Parameters to check.\n",
    "        model_params = ['U','V','W']\n",
    "        ## Perform 'manual' check on each parameter.\n",
    "        for pid, pname in enumerate(model_params):\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print 'Performing gradient check on %s with size %d.' %(pname, np.prod(parameter.shape))\n",
    "            ## Iterate over elements of the parameter matrix.\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                ## Store original value.\n",
    "                original_value = parameter[ix]\n",
    "                ## Estimate gradient manually.\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.total_loss_function([x], [y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = model.total_loss_function([x], [y])\n",
    "                estimated_gradient = (gradplus - gradminus) / (2*h)\n",
    "                ## Reset parameter to original value.\n",
    "                parameter[ix] = original_value\n",
    "                ## Estimate gradient with bptt.\n",
    "                backprop_gradient = bptt_gradients[pid][ix]\n",
    "                ## Calculate relative error.\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient) / \\\n",
    "                (np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                ## If the error is to large, do not pass the test.\n",
    "                if relative_error > error_threshold:\n",
    "                    print 'Gradient check ERROR: parameter=%s, index=%s.' %(pname, ix)\n",
    "                    print 'Relative Error: %f.' %relative_error\n",
    "                it.iternext()\n",
    "            print 'Gradient check for parameter %s passed! :)' %pname\n",
    "            \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        ## Calculate gradients with bptt.\n",
    "        dLdU, dLdV, dLdW = self.bptt(x,y)\n",
    "        ## Update according to gradient and learning rate.\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, epochs=100, evaluate_loss_after=5):\n",
    "    ## List to keep track of losses.\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(epochs):\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            loss = model.loss_function(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print '%s: Loss after num_examples=%d & epoch=%d: %f.' %(time, num_examples_seen, epoch, loss)\n",
    "            ## Adjust learning rate if loss increases.\n",
    "            if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print 'Setting learning rate to %f.' %learning_rate\n",
    "            sys.stdout.flush()\n",
    "            ## For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            ## do one SGD step.\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.07 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100 loops, best of 3: 3.74 ms per loop\n"
     ]
    }
   ],
   "source": [
    "## Running time of one SGD.\n",
    "model = RNNNumpy(size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-07 16:49:50: Loss after num_examples=0 & epoch=0: 6.908023.\n",
      "2016-08-07 16:51:25: Loss after num_examples=15576 & epoch=1: 5.796630.\n",
      "2016-08-07 16:52:48: Loss after num_examples=31152 & epoch=2: 5.662541.\n",
      "2016-08-07 16:54:11: Loss after num_examples=46728 & epoch=3: 5.596979.\n",
      "2016-08-07 16:55:52: Loss after num_examples=62304 & epoch=4: 5.553559.\n",
      "2016-08-07 16:58:02: Loss after num_examples=77880 & epoch=5: 5.520663.\n",
      "2016-08-07 17:00:07: Loss after num_examples=93456 & epoch=6: 5.493906.\n",
      "2016-08-07 17:02:18: Loss after num_examples=109032 & epoch=7: 5.471182.\n",
      "2016-08-07 17:04:04: Loss after num_examples=124608 & epoch=8: 5.451323.\n",
      "2016-08-07 17:06:08: Loss after num_examples=140184 & epoch=9: 5.433628.\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(size)\n",
    "losses = train_with_sgd(model, X_train, y_train, epochs=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.00080787,  0.00083021,  0.00083636,  0.00081357,  0.0007669 ,\n",
      "         0.00073279,  0.00083247,  0.0009255 ,  0.00068345,  0.00062778,\n",
      "         0.00079672,  0.00077602,  0.00075505,  0.00077087,  0.00078   ,\n",
      "         0.00095363,  0.00062058,  0.0009512 ,  0.00091348,  0.00088188,\n",
      "         0.00074998,  0.00078706,  0.00096275,  0.00083992,  0.00083765,\n",
      "         0.00072543,  0.00078783,  0.00088986,  0.00079196,  0.00085697,\n",
      "         0.00081824,  0.00071025,  0.00082146,  0.00074054,  0.00071955,\n",
      "         0.00070281,  0.00083039,  0.00068357,  0.000626  ,  0.00053404,\n",
      "         0.00094485,  0.0008237 ,  0.00084   ,  0.00087343,  0.00079416,\n",
      "         0.0007064 ,  0.0007741 ,  0.0008745 ,  0.00088776,  0.00073298,\n",
      "         0.00079112,  0.00087247,  0.00096041,  0.00066232,  0.00094743,\n",
      "         0.00087488,  0.00104881,  0.00074727,  0.00087617,  0.00103789,\n",
      "         0.00098202,  0.00066384,  0.00078333,  0.0008003 ,  0.00087563,\n",
      "         0.00072566,  0.0008505 ,  0.0008174 ,  0.00077316,  0.00075903,\n",
      "         0.00079825,  0.00067341,  0.00097236,  0.00070723,  0.00069046,\n",
      "         0.00079592,  0.0007661 ,  0.00089165,  0.00098001,  0.00078155,\n",
      "         0.00072979,  0.00081377,  0.00084296,  0.00084111,  0.00101316,\n",
      "         0.00078026,  0.00073602,  0.00094738,  0.00082219,  0.00074936,\n",
      "         0.0008884 ,  0.00093801,  0.00081358,  0.00084257,  0.00074051,\n",
      "         0.00076766,  0.00070347,  0.00089923,  0.00101952,  0.00076743,\n",
      "         0.00091815,  0.00074459,  0.0007218 ,  0.00085601,  0.0009883 ,\n",
      "         0.00058031,  0.00092398,  0.00074372,  0.00076974,  0.00083299,\n",
      "         0.00100306,  0.00076045,  0.00087188,  0.00072536,  0.00074433,\n",
      "         0.00097306,  0.00101385,  0.00082109,  0.00100785,  0.00086912,\n",
      "         0.00088123,  0.00100049,  0.00101694,  0.00084477,  0.00098774,\n",
      "         0.00076975,  0.00065588,  0.0006817 ,  0.00097292,  0.00069488,\n",
      "         0.00068081,  0.0007395 ,  0.00077398,  0.00090164,  0.00064305,\n",
      "         0.00109385,  0.0007574 ,  0.00065399,  0.00095196,  0.00096209,\n",
      "         0.00094599,  0.00071753,  0.0006538 ,  0.00109385,  0.00092465,\n",
      "         0.00092487,  0.00081168,  0.00080845,  0.00078639,  0.00098417,\n",
      "         0.00080281,  0.0007808 ,  0.00074641,  0.00100057,  0.00083734,\n",
      "         0.00077255,  0.00098913,  0.00080845,  0.0008641 ,  0.00075541,\n",
      "         0.00087825,  0.00076432,  0.00068849,  0.00067997,  0.0007071 ,\n",
      "         0.00080266,  0.0006715 ,  0.00085092,  0.00095144,  0.00085735,\n",
      "         0.00083819,  0.00098585,  0.00082529,  0.00080397,  0.00067993,\n",
      "         0.00070154,  0.00082671,  0.00078813,  0.00068673,  0.00070136,\n",
      "         0.00086683,  0.00091947,  0.00090939,  0.00082229,  0.00083741,\n",
      "         0.00084738,  0.00086104,  0.00076961,  0.00097287,  0.00100063,\n",
      "         0.00109671,  0.0009655 ,  0.00096749,  0.00101199,  0.00062272,\n",
      "         0.00073462,  0.00073667,  0.00063062,  0.00070618,  0.0009593 ,\n",
      "         0.00074394,  0.00086327,  0.0010294 ,  0.00082633,  0.00084961,\n",
      "         0.00105272,  0.00077892,  0.00071991,  0.00081026,  0.00103378,\n",
      "         0.00080275,  0.00074602,  0.0008188 ,  0.00072131,  0.00085431,\n",
      "         0.00078447,  0.0007411 ,  0.00073938,  0.00093929,  0.00094667,\n",
      "         0.00075372,  0.00092984,  0.00097405,  0.00090549,  0.00068647,\n",
      "         0.00067076,  0.00087802,  0.00072087,  0.00074922,  0.00080215,\n",
      "         0.00095156,  0.00075807,  0.0007202 ,  0.00087235,  0.00097975,\n",
      "         0.00077138,  0.00075878,  0.00084447,  0.00091002,  0.0009674 ,\n",
      "         0.0006196 ,  0.00064324,  0.00073626,  0.00075547,  0.00081053,\n",
      "         0.0007452 ,  0.00066897,  0.00093292,  0.00079326,  0.00081475,\n",
      "         0.00065802,  0.00067027,  0.00063169,  0.00081802,  0.00081774,\n",
      "         0.00095685,  0.00085241,  0.00094148,  0.00083112,  0.00076856,\n",
      "         0.00081973,  0.00065184,  0.00099357,  0.0007101 ,  0.00089205,\n",
      "         0.00087076,  0.00072104,  0.0009089 ,  0.00076996,  0.00069491,\n",
      "         0.00076235,  0.00100439,  0.0007521 ,  0.00076944,  0.00074211,\n",
      "         0.00069299,  0.0007812 ,  0.00068526,  0.00089565,  0.00087031,\n",
      "         0.00095055,  0.00074588,  0.00082502,  0.00077073,  0.00101953,\n",
      "         0.00068046,  0.00095319,  0.00082903,  0.00089392,  0.00076227,\n",
      "         0.00079703,  0.00061876,  0.0007872 ,  0.00087171,  0.00094454,\n",
      "         0.00081862,  0.00091957,  0.00079545,  0.00086409,  0.00077767,\n",
      "         0.00097736,  0.00075869,  0.00077217,  0.00087822,  0.00084032,\n",
      "         0.00083979,  0.0007785 ,  0.00075413,  0.00083064,  0.00095806,\n",
      "         0.00099386,  0.00069793,  0.00072634,  0.00081753,  0.00066808,\n",
      "         0.00073685,  0.00085531,  0.00076151,  0.00084469,  0.00082568,\n",
      "         0.0009162 ,  0.00081292,  0.0007    ,  0.0007813 ,  0.00077733,\n",
      "         0.00079159,  0.00079107,  0.00065451,  0.00075162,  0.00111194,\n",
      "         0.00094177,  0.00065795,  0.00063596,  0.00069332,  0.00064466,\n",
      "         0.0005724 ,  0.00071271,  0.00078376,  0.00089964,  0.00090425,\n",
      "         0.00075156,  0.00097543,  0.0006662 ,  0.00093997,  0.00067869,\n",
      "         0.00082336,  0.00091391,  0.0007044 ,  0.00082392,  0.00077908,\n",
      "         0.00080182,  0.00085914,  0.0008232 ,  0.00103244,  0.00085994,\n",
      "         0.00074835,  0.00104899,  0.00081277,  0.00082819,  0.00087152,\n",
      "         0.00072347,  0.00078803,  0.00077953,  0.00070386,  0.00070955,\n",
      "         0.00082521,  0.00070413,  0.00079072,  0.00110434,  0.00078064,\n",
      "         0.0009971 ,  0.00085449,  0.00086823,  0.00101194,  0.00072417,\n",
      "         0.00085392,  0.00068158,  0.00096192,  0.00090287,  0.00088646,\n",
      "         0.00079119,  0.0009661 ,  0.00066966,  0.00077208,  0.00072847,\n",
      "         0.00093496,  0.00093888,  0.00073864,  0.00087377,  0.00062132,\n",
      "         0.00079429,  0.00101218,  0.00056475,  0.00083329,  0.00077438,\n",
      "         0.00081653,  0.00074359,  0.00085217,  0.00095201,  0.0008339 ,\n",
      "         0.00071526,  0.00082798,  0.00084927,  0.00088543,  0.00074404,\n",
      "         0.00065253,  0.00090709,  0.00078422,  0.00084248,  0.00082062,\n",
      "         0.00069272,  0.00088771,  0.00077377,  0.00094155,  0.0007968 ,\n",
      "         0.00061195,  0.00063456,  0.00057835,  0.00068265,  0.00068834,\n",
      "         0.00077201,  0.00088494,  0.00091167,  0.00079095,  0.00089582,\n",
      "         0.00067545,  0.0007316 ,  0.00093564,  0.00072373,  0.00076749,\n",
      "         0.0011282 ,  0.00075105,  0.00078332,  0.00086416,  0.00082979,\n",
      "         0.00089586,  0.00093063,  0.00079789,  0.00066641,  0.0008859 ,\n",
      "         0.00075679,  0.00090877,  0.00072517,  0.00081506,  0.00076948,\n",
      "         0.00073363,  0.00080334,  0.00084249,  0.00101584,  0.00092318,\n",
      "         0.0008501 ,  0.0011469 ,  0.00071508,  0.00097739,  0.00093049,\n",
      "         0.00062763,  0.00065753,  0.00089153,  0.00077039,  0.00096383,\n",
      "         0.00088006,  0.00076345,  0.00082521,  0.00077611,  0.00082699,\n",
      "         0.00089572,  0.00092489,  0.00080939,  0.00081014,  0.00085006,\n",
      "         0.00069672,  0.00071338,  0.00071325,  0.00082124,  0.00096227,\n",
      "         0.00079013,  0.00078693,  0.0007917 ,  0.00073367,  0.00102162,\n",
      "         0.00068978,  0.00069043,  0.00067115,  0.00059514,  0.00063557,\n",
      "         0.00089599,  0.00072721,  0.00095508,  0.00096906,  0.00078596,\n",
      "         0.00087726,  0.00075417,  0.00078864,  0.00080394,  0.00083594,\n",
      "         0.00079329,  0.00093193,  0.00099889,  0.00085893,  0.00086801,\n",
      "         0.0006041 ,  0.00073599,  0.00066019,  0.00089668,  0.00059635,\n",
      "         0.00076611,  0.00085201,  0.00083266,  0.0007721 ,  0.00095456,\n",
      "         0.00098803,  0.00071182,  0.00072784,  0.00075287,  0.00092044,\n",
      "         0.00086496,  0.0010564 ,  0.00096383,  0.0007214 ,  0.00095804,\n",
      "         0.0008491 ,  0.0008613 ,  0.0007255 ,  0.00081403,  0.00090551,\n",
      "         0.00096066,  0.00089625,  0.00095331,  0.00083508,  0.00073421,\n",
      "         0.00074029,  0.00064815,  0.00080186,  0.00071618,  0.00072999,\n",
      "         0.00089076,  0.00081714,  0.00083327,  0.00085956,  0.00070609,\n",
      "         0.00106531,  0.00086142,  0.00088117,  0.00073445,  0.0012044 ,\n",
      "         0.00099302,  0.00085647,  0.00084978,  0.00060007,  0.00091559,\n",
      "         0.00072228,  0.00074138,  0.00080242,  0.00085607,  0.00072759,\n",
      "         0.00078755,  0.00081613,  0.00072326,  0.0010112 ,  0.00078092,\n",
      "         0.00070598,  0.00082601,  0.00092464,  0.00092866,  0.00073785,\n",
      "         0.00079019,  0.00078315,  0.00089587,  0.00094942,  0.00070214,\n",
      "         0.00087549,  0.00091323,  0.00094563,  0.00097955,  0.00083128,\n",
      "         0.00077096,  0.0009848 ,  0.00082897,  0.00081093,  0.00076795,\n",
      "         0.00067586,  0.00091158,  0.00074919,  0.00084835,  0.00077958,\n",
      "         0.00072962,  0.00074149,  0.000844  ,  0.00101253,  0.00096803,\n",
      "         0.00080244,  0.00079479,  0.00087841,  0.00067659,  0.00091111,\n",
      "         0.00091384,  0.00095443,  0.00080474,  0.00061517,  0.00076034,\n",
      "         0.00095318,  0.00096036,  0.00101073,  0.00080261,  0.00073293,\n",
      "         0.00075015,  0.00085566,  0.00072843,  0.00081237,  0.00098538,\n",
      "         0.00101992,  0.00081529,  0.00071861,  0.00088415,  0.00085486,\n",
      "         0.00087032,  0.00082114,  0.00086796,  0.00096166,  0.00084602,\n",
      "         0.00076581,  0.00077079,  0.00081165,  0.0008031 ,  0.0007117 ,\n",
      "         0.00079449,  0.00087967,  0.00085636,  0.00079986,  0.00082213,\n",
      "         0.00075784,  0.00068633,  0.00084492,  0.00065306,  0.00060527,\n",
      "         0.00085411,  0.00100249,  0.00084836,  0.0008898 ,  0.00070801,\n",
      "         0.00094704,  0.00073892,  0.00089575,  0.00091727,  0.00068703,\n",
      "         0.00064384,  0.00060469,  0.00084106,  0.00088733,  0.00078568,\n",
      "         0.00106856,  0.00073524,  0.0007791 ,  0.00078861,  0.00062981,\n",
      "         0.00059381,  0.00089127,  0.00070541,  0.00084168,  0.0006427 ,\n",
      "         0.00087073,  0.00075698,  0.00073264,  0.00084223,  0.00090146,\n",
      "         0.00077275,  0.00089761,  0.00074544,  0.00072332,  0.0007489 ,\n",
      "         0.00072402,  0.00074881,  0.00100991,  0.00074687,  0.00084721,\n",
      "         0.0009089 ,  0.00073129,  0.00065728,  0.00067139,  0.00076134,\n",
      "         0.00087788,  0.00087855,  0.00068867,  0.00082731,  0.0009094 ,\n",
      "         0.00090382,  0.00065574,  0.00092687,  0.00071131,  0.00086297,\n",
      "         0.00092221,  0.00068316,  0.00069415,  0.00081858,  0.00065232,\n",
      "         0.00067994,  0.000798  ,  0.00077101,  0.00089949,  0.0007435 ,\n",
      "         0.0009138 ,  0.00092075,  0.00088434,  0.00097432,  0.00075057,\n",
      "         0.0010546 ,  0.00087777,  0.00076738,  0.00083888,  0.00070777,\n",
      "         0.00088801,  0.00083367,  0.00105105,  0.00062091,  0.00066578,\n",
      "         0.00090303,  0.00093918,  0.00061566,  0.0006939 ,  0.00075712,\n",
      "         0.00087274,  0.00110641,  0.00061666,  0.00090883,  0.00078775,\n",
      "         0.00070858,  0.00106499,  0.00077993,  0.00076388,  0.00061727,\n",
      "         0.00091439,  0.00093988,  0.00062764,  0.00081009,  0.00085525,\n",
      "         0.00079734,  0.0007779 ,  0.00069612,  0.00079742,  0.00104108,\n",
      "         0.00089182,  0.00080375,  0.00069933,  0.00102978,  0.00065659,\n",
      "         0.00103408,  0.00067058,  0.00073173,  0.00091388,  0.00068658,\n",
      "         0.00096963,  0.00077009,  0.0007731 ,  0.00068628,  0.00086301,\n",
      "         0.00053893,  0.00074905,  0.00077549,  0.00093447,  0.00064835,\n",
      "         0.00061512,  0.00078612,  0.00096072,  0.00092724,  0.00100419,\n",
      "         0.00091182,  0.00090287,  0.00090348,  0.00096209,  0.00083178,\n",
      "         0.00104511,  0.00070287,  0.00103897,  0.00076845,  0.00073044,\n",
      "         0.00120207,  0.00083199,  0.00089593,  0.00099234,  0.00079918,\n",
      "         0.00078475,  0.00067146,  0.00076299,  0.00096299,  0.00088969,\n",
      "         0.00073837,  0.00091055,  0.00076469,  0.00079019,  0.00063682,\n",
      "         0.00081877,  0.00092111,  0.00071557,  0.00082022,  0.00075449,\n",
      "         0.00081912,  0.00088978,  0.00071549,  0.0008425 ,  0.0010579 ,\n",
      "         0.00084403,  0.0007278 ,  0.00089331,  0.00080296,  0.00095498,\n",
      "         0.00087263,  0.0010626 ,  0.00081609,  0.00089364,  0.00085114,\n",
      "         0.00098314,  0.00069254,  0.00080889,  0.00083103,  0.00079086,\n",
      "         0.0007607 ,  0.00061516,  0.00077421,  0.00077434,  0.00086552,\n",
      "         0.00081268,  0.00094211,  0.00094319,  0.00080822,  0.00066658,\n",
      "         0.0008216 ,  0.00086955,  0.00080943,  0.00111521,  0.00073787,\n",
      "         0.00084639,  0.00074972,  0.0007578 ,  0.00071485,  0.0007419 ,\n",
      "         0.00069911,  0.0004723 ,  0.00074554,  0.00093326,  0.00114441,\n",
      "         0.00061585,  0.00071768,  0.00095338,  0.00068244,  0.00087203,\n",
      "         0.00080209,  0.00096053,  0.00075506,  0.00089019,  0.0006778 ,\n",
      "         0.00063591,  0.00070871,  0.00071101,  0.00073556,  0.00095583,\n",
      "         0.00083873,  0.00086002,  0.00089482,  0.00108099,  0.00082774,\n",
      "         0.00085167,  0.0005958 ,  0.00073078,  0.00070029,  0.00078597,\n",
      "         0.00084519,  0.0008871 ,  0.00066184,  0.00090013,  0.00093322,\n",
      "         0.00104304,  0.00077512,  0.00069365,  0.00079947,  0.00085621,\n",
      "         0.00084472,  0.00077382,  0.00081466,  0.00087242,  0.00067232,\n",
      "         0.00087243,  0.00082844,  0.00059916,  0.00069764,  0.00071481,\n",
      "         0.00092494,  0.00066303,  0.00083261,  0.0008785 ,  0.00061454,\n",
      "         0.00076813,  0.00099426,  0.00078291,  0.00091687,  0.00078313,\n",
      "         0.00069882,  0.00084101,  0.00076107,  0.00069889,  0.00087059,\n",
      "         0.00065501,  0.00072133,  0.00080475,  0.00103796,  0.00071037,\n",
      "         0.00085052,  0.00080302,  0.00065104,  0.00104073,  0.00078753,\n",
      "         0.00081514,  0.00065503,  0.00080024,  0.00081064,  0.00070674,\n",
      "         0.0006211 ,  0.00097292,  0.00102007,  0.00086512,  0.00084898,\n",
      "         0.00081489,  0.00094708,  0.00074286,  0.00109446,  0.00085667,\n",
      "         0.0007299 ,  0.00085901,  0.0010523 ,  0.00078527,  0.00076028,\n",
      "         0.0008749 ,  0.00074728,  0.00086217,  0.0008219 ,  0.0008072 ,\n",
      "         0.00074571,  0.00075031,  0.00084824,  0.00065834,  0.0008957 ,\n",
      "         0.00063647,  0.00095185,  0.00100696,  0.00083543,  0.00083129,\n",
      "         0.00102822,  0.00079705,  0.00100882,  0.00071747,  0.00058714,\n",
      "         0.00076505,  0.00082249,  0.00073538,  0.00082795,  0.00063147,\n",
      "         0.00068767,  0.00093166,  0.00084919,  0.00070154,  0.00068405,\n",
      "         0.00074944,  0.00076793,  0.00079234,  0.00069991,  0.00077382,\n",
      "         0.00096817,  0.00077831,  0.00087153,  0.00070073,  0.00068549,\n",
      "         0.00094373,  0.00060037,  0.00065034,  0.00085788,  0.0009678 ,\n",
      "         0.00074957,  0.00096805,  0.00081634,  0.0008258 ,  0.00100894,\n",
      "         0.00076448,  0.00083326,  0.00091419,  0.00087848,  0.00082467,\n",
      "         0.00082769,  0.00086014,  0.00075589,  0.0009293 ,  0.00084688,\n",
      "         0.0011358 ,  0.000971  ,  0.00058646,  0.00072803,  0.00065433,\n",
      "         0.00094404,  0.00081118,  0.00077236,  0.00093088,  0.18272852]]), array([[ -2.30733354e-01,  -4.63734346e-01,   1.01392926e-01,\n",
      "         -2.36936697e-01,  -6.46606850e-02,   5.90830864e-02,\n",
      "          1.09867683e-03,  -2.54788879e-01,   1.36928820e-01,\n",
      "          1.85103992e-02,  -1.11696273e-01,  -2.23850083e-01,\n",
      "          2.39134183e-01,   2.22533495e-01,   1.39387331e-01,\n",
      "         -2.13039182e-01,  -1.03414737e-01,  -2.02399440e-01,\n",
      "          2.86761888e-01,   1.95114117e-01,   2.17581376e-01,\n",
      "         -1.31303816e-01,  -1.95564354e-02,  -2.34487233e-01,\n",
      "          3.38918930e-01,  -9.33540156e-02,   1.63195116e-01,\n",
      "         -3.00806996e-02,   2.29046853e-01,  -3.35195847e-02,\n",
      "         -6.00309342e-02,  -4.23540633e-01,   4.93563148e-01,\n",
      "         -4.12285940e-02,  -8.86775901e-02,   2.61002253e-01,\n",
      "          2.06612566e-01,  -7.43122657e-02,   4.07735689e-04,\n",
      "          4.11391643e-01,  -2.05244438e-01,  -3.13803361e-01,\n",
      "          1.92887818e-01,  -2.66296696e-01,  -3.34468540e-01,\n",
      "          3.13113091e-01,  -6.59915518e-02,   6.15554982e-02,\n",
      "          1.38813427e-01,  -4.48931600e-02,   2.29704295e-01,\n",
      "         -1.22998460e-01,   5.79884772e-01,   3.33341938e-01,\n",
      "         -2.14331940e-01,  -4.75582112e-01,   1.51122154e-01,\n",
      "          2.42620601e-01,   1.22042325e-01,  -2.60285046e-01,\n",
      "          4.71572064e-02,   4.06919804e-02,   2.21617740e-02,\n",
      "          1.97862813e-01,   4.95209787e-01,  -4.56697006e-01,\n",
      "         -3.34999888e-01,  -2.85158795e-01,  -1.17474153e-01,\n",
      "         -3.24313485e-01,  -2.37708957e-01,   1.52458829e-01,\n",
      "         -3.94924799e-01,   9.70789859e-02,  -4.92936525e-02,\n",
      "          2.35662692e-02,   3.34428392e-02,  -2.04278479e-01,\n",
      "         -3.24020489e-02,  -1.06074921e-01,   2.15435301e-02,\n",
      "          2.42639152e-01,  -2.28757750e-01,   2.77434422e-02,\n",
      "          5.49035588e-02,  -2.91319296e-01,   2.08199971e-01,\n",
      "         -3.27601915e-01,  -3.21651315e-01,  -7.07252627e-02,\n",
      "          2.66526275e-01,   3.60876020e-01,  -3.89607252e-01,\n",
      "         -2.83289459e-01,  -1.22392993e-03,   3.63742255e-01,\n",
      "          8.83675019e-02,  -8.07384312e-02,  -9.33118915e-03,\n",
      "          3.78543782e-01],\n",
      "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d6c3a8413e89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# We want long sentences, not sentences with one or two words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msenten_min_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-d6c3a8413e89>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0msampled_word\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munknown_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[0mnext_word_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_word_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0msampled_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mnew_sentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy/random/mtrand/mtrand.c:31245)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_index[start_symbol]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_index[stop_symbol]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_index[unknown_token]:\n",
    "            print next_word_probs\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [word_list[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n",
      "[[ 0.00100879  0.00098313  0.00100766 ...,  0.00100895  0.00098688\n",
      "   0.00101433]\n",
      " [ 0.00100893  0.00098647  0.00100164 ...,  0.0010173   0.00102306\n",
      "   0.0009967 ]\n",
      " [ 0.00101677  0.00099818  0.00099919 ...,  0.0010082   0.00101229\n",
      "   0.00098475]\n",
      " [ 0.00098407  0.00099687  0.00100374 ...,  0.00098535  0.00100829\n",
      "   0.00100489]\n",
      " [ 0.00098409  0.00099108  0.00100633 ...,  0.00098135  0.0009985\n",
      "   0.00097782]]\n",
      "(5,)\n",
      "[1, 999, 999, 39, 10]\n",
      "[  5 940 251 513  66]\n",
      "\n",
      "Expected Loss for random predictions: 6.907755\n",
      "Actual loss: 6.90680664384\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o\n",
    "\n",
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print X_train[10]\n",
    "print predictions\n",
    "\n",
    "\n",
    "print \"\\nExpected Loss for random predictions: %f\" % np.log(size)\n",
    "print 'Actual loss:', model.loss_function(X_train[:500], y_train[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
