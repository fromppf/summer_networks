{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Bukowski\n",
    "Following Denny's deep neural network tutorial on [WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/), I try to build a  Recurrent Neural Network that can write poems like Bukowski did. For this I take a broad sample of his poems (??? in total) and use them as input data. On this initial section all the implementation is in numpy, and the goal is to generate a couple of sentences that look like Buk's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import libaries.\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "First step is to read the poem files, store them in a convenient location (pandas DF) and keep some labeled information that might be useful in the future. The words on the lines of the poems will be okenized, and each word will receive an index value more easily consumable by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file Bukowski_1974_77.txt\n",
      "Processing file Bukowski_1960_97.txt\n",
      "Processed a total of 15576 lines.\n"
     ]
    }
   ],
   "source": [
    "size = 1000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "start_symbol = 'START_SYMBOL'\n",
    "stop_symbol = 'STOP_SYMBOL'\n",
    "\n",
    "## Collect input files.\n",
    "files = []\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\"7.txt\"):\n",
    "        files.append(file)\n",
    "\n",
    "## Create empty data frame.\n",
    "content = pd.DataFrame({'line' : [], 'text' : [], 'tokens' : [], 'poem' : []})\n",
    "\n",
    "## Fill dataframe from files.\n",
    "for file in files:\n",
    "    print \"Processing file\", file\n",
    "    for l in open(file, 'rb'):\n",
    "        line = l.strip('\\n')\n",
    "        if l[0].isdigit():\n",
    "            data = line.split('   ')\n",
    "            if len(data) > 1:\n",
    "                sentence = start_symbol + ' ' + data[1] + ' ' + stop_symbol\n",
    "                tokens = nltk.word_tokenize(sentence.decode('utf-8').lower())\n",
    "                content = content.append({'line' : int(data[0]), 'text' : data[1], 'tokens' : tokens}, ignore_index = True)\n",
    "\n",
    "## Make the 'line' content numeric \n",
    "content['line'] = pd.to_numeric(content['line'])\n",
    "print \"Processed a total of %d lines.\" %content.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 334 distinct poems.\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "poem = 1\n",
    "for i, row in content.iterrows():\n",
    "    if row['line'] >= count:\n",
    "        count = row['line']\n",
    "        content.loc[i, 'poem'] = poem\n",
    "    else:\n",
    "        count = 1\n",
    "        poem += 1\n",
    "        content.loc[i, 'poem'] = poem\n",
    "## Store results on csv.        \n",
    "content.to_csv('content.csv', index = False)\n",
    "print \"Parsed %d distinct poems.\" %poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6515 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "## Get word frequency.\n",
    "sentences = [content.loc[i, 'tokens'] for i in content.index]\n",
    "flattened = itertools.chain.from_iterable(sentences) \n",
    "word_freq = nltk.FreqDist(flattened)\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1000.\n",
      "The least frequent word in our vocabulary is 'victory' and appeared 7 times.\n"
     ]
    }
   ],
   "source": [
    "## Limit vocabulary to most common words.\n",
    "vocab = word_freq.most_common(size-1)\n",
    "word_list = [x[0] for x in vocab]\n",
    "word_list.append(unknown_token)\n",
    "word_list\n",
    "word_index = dict([(w,i) for i,w in enumerate(word_list)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Replace words not in dictionary with the unknown token.\n",
    "for i, row in content.iterrows():\n",
    "    for j, tkn in enumerate(row['tokens']):\n",
    "        if tkn not in word_list:\n",
    "            content.loc[i, 'tokens'][j] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input data:  start_symbol you 're a unknown_token , she said\n",
      "Example Output data: you 're a unknown_token , she said stop_symbol\n"
     ]
    }
   ],
   "source": [
    "## Convert the info into training data.\n",
    "X_train = np.asarray([[word_index[w] for w in sent[:-1]] for sent in content['tokens'].tolist()])\n",
    "y_train = np.asarray([[word_index[w] for w in sent[1:]] for sent in content['tokens'].tolist()])\n",
    "\n",
    "## Show an example.\n",
    "x_example, y_example = X_train[49], y_train[49]\n",
    "print 'Example Input data: ', \n",
    "input_sent = [word_list[i] for i in x_example]\n",
    "print ' '.join(x for x in input_sent)\n",
    "\n",
    "print 'Example Output data:', \n",
    "output_sent = [word_list[j] for j in y_example]\n",
    "print ' '.join(x for x in output_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "Now that we have the data in the desired format, we start building the neural network. Our **RNNNumpy** class is made up by the following functions:\n",
    "\n",
    "**__init__**: Initializes parameters based on the number of word dimensions, hidden dimensions and the uniform distribution $ \\left[\\dfrac{-1}{\\sqrt{n}}, \\dfrac{1}{\\sqrt{n}}\\right] $ for (U, V & W).\n",
    "\n",
    "**forward_propagation**: What the name implies, using a *tanh* activation function for the input layer and *softmax* for the output one.\n",
    "\n",
    "**predict**: Returns output with highest probability from the forward propagation estimates.\n",
    "\n",
    "**calculate_loss**: Cross entropy loss function, as defined below.\n",
    "\n",
    "$ L(y,o) = \\dfrac{-1}{N} \\sum y_n log(o_n)$\n",
    "\n",
    "**bptt**: Back propagation algorithm, used to estimate parameters.\n",
    "\n",
    "**gradient_check**: Safety measure for validating that our BPPT estimates are going in the right direction.\n",
    "\n",
    "$ \\dfrac{\\partial L}{\\partial \\theta} \\approx J(\\theta + h) - \\dfrac{J(\\theta + h)}{2 h} $\n",
    "\n",
    "**sgd_step**: SGD to calculate gradients and perform updates in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define our softmax function.\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self, w_dim, h_dim=100, bptt_max=4):\n",
    "        self.w_dim = w_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.bptt_max = bptt_max\n",
    "        ## Randomly initialize network parameters.\n",
    "        ## Set to uniform between [-1/n, 1/n], where\n",
    "        ## 'n' is the size of incoming connections.\n",
    "        self.U = np.random.uniform(-np.sqrt(1./w_dim), np.sqrt(1./w_dim), (h_dim, w_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (w_dim, h_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (h_dim, h_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        ## Length of inputs.\n",
    "        T = len(x)\n",
    "        ## Generate matrix for all hidden states and all outputs.\n",
    "        s = np.zeros((T+1, self.h_dim))\n",
    "        o = np.zeros((T, self.w_dim))\n",
    "        ## Iterate though the steps.\n",
    "        for t in np.arange(T):\n",
    "            # s[t] = Ux[t] = Ws[t-1]\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            # o[t] = softmax(Vs[t])\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def total_loss_function(self, x, y):\n",
    "        L = 0\n",
    "        ## Calculate loss on each sentence.\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            ## Select predictions for correct words.\n",
    "            correct_preds = o[np.arange(len(y[i])), y[i]]\n",
    "            L -= np.sum(np.log(correct_preds))\n",
    "        return L\n",
    "    \n",
    "    def loss_function(self, x, y):\n",
    "        N = np.sum(len(y_i) for y_i in y)\n",
    "        return self.total_loss_function(x,y)/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        ## Start with forward-propagation.\n",
    "        o, s = self.forward_propagation(x)\n",
    "        dLdU = np.zeros_like(self.U)\n",
    "        dLdV = np.zeros_like(self.V)\n",
    "        dLdW = np.zeros_like(self.W)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1\n",
    "        ## Go over observations, from end to start.\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV = np.outer(delta_o[t], s[t].T)\n",
    "            ## Initial delta.\n",
    "            delta_t = np.inner(self.V.T, delta_o[t]) * (1 - s[t]**2)\n",
    "            ## Now we do the back-propagation.\n",
    "            for step in np.arange(max(0, t - self.bptt_max), t+1)[::-1]:\n",
    "                dLdW = np.outer(delta_t, s[step - 1]) \n",
    "                dLdU[:, x[step]] += delta_t\n",
    "                ## Update delta for next iteration.\n",
    "                delta_t = np.inner(self.W.T, delta_t) * (1 - s[step - 1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        ## Calculate the parameters with bptt.\n",
    "        bptt_gradients = model.bptt(x,y)\n",
    "        ## Parameters to check.\n",
    "        model_params = ['U','V','W']\n",
    "        ## Perform 'manual' check on each parameter.\n",
    "        for pid, pname in enumerate(model_params):\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print 'Performing gradient check on %s with size %d.' %(pname, np.prod(parameter.shape))\n",
    "            ## Iterate over elements of the parameter matrix.\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                ## Store original value.\n",
    "                original_value = parameter[ix]\n",
    "                ## Estimate gradient manually.\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.total_loss_function([x], [y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = model.total_loss_function([x], [y])\n",
    "                estimated_gradient = (gradplus - gradminus) / (2*h)\n",
    "                ## Reset parameter to original value.\n",
    "                parameter[ix] = original_value\n",
    "                ## Estimate gradient with bptt.\n",
    "                backprop_gradient = bptt_gradients[pid][ix]\n",
    "                ## Calculate relative error.\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient) / \\\n",
    "                (np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                ## If the error is to large, do not pass the test.\n",
    "                if relative_error > error_threshold:\n",
    "                    print 'Gradient check ERROR: parameter=%s, index=%s.' %(pname, ix)\n",
    "                    print 'Relative Error: %f.' %relative_error\n",
    "                it.iternext()\n",
    "            print 'Gradient check for parameter %s passed! :)' %pname\n",
    "            \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        ## Calculate gradients with bptt.\n",
    "        dLdU, dLdV, dLdW = self.bptt(x,y)\n",
    "        ## Update according to gradient and learning rate.\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, epochs=100, evaluate_loss_after=5):\n",
    "    ## List to keep track of losses.\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(epochs):\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            loss = model.loss_function(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print '%s: Loss after num_examples=%d & epoch=%d: %f.' %(time, num_examples_seen, epoch, loss)\n",
    "            ## Adjust learning rate if loss increases.\n",
    "            if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print 'Setting learning rate to %f.' %learning_rate\n",
    "            sys.stdout.flush()\n",
    "            ## For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            ## do one SGD step.\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 9.43 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100 loops, best of 3: 4.26 ms per loop\n"
     ]
    }
   ],
   "source": [
    "## Running time of one SGD.\n",
    "model = RNNNumpy(size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last test gives us an indication of the total running time for the algorithm, if we are to use the full input data. Without optimizing the cost this will be about ??? days (quite long). Using theano should decrease the running time by a large factor.\n",
    "\n",
    "At this point the algorithm is complete, we can perform a short test with the `train_with_sgd` function to verify that indeed our BPTT gradient descend is working (the error should drop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-07 16:49:50: Loss after num_examples=0 & epoch=0: 6.908023.\n",
      "2016-08-07 16:51:25: Loss after num_examples=15576 & epoch=1: 5.796630.\n",
      "2016-08-07 16:52:48: Loss after num_examples=31152 & epoch=2: 5.662541.\n",
      "2016-08-07 16:54:11: Loss after num_examples=46728 & epoch=3: 5.596979.\n",
      "2016-08-07 16:55:52: Loss after num_examples=62304 & epoch=4: 5.553559.\n",
      "2016-08-07 16:58:02: Loss after num_examples=77880 & epoch=5: 5.520663.\n",
      "2016-08-07 17:00:07: Loss after num_examples=93456 & epoch=6: 5.493906.\n",
      "2016-08-07 17:02:18: Loss after num_examples=109032 & epoch=7: 5.471182.\n",
      "2016-08-07 17:04:04: Loss after num_examples=124608 & epoch=8: 5.451323.\n",
      "2016-08-07 17:06:08: Loss after num_examples=140184 & epoch=9: 5.433628.\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(size)\n",
    "train_with_sgd(model, X_train, y_train, epochs=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sentences\n",
    "We can use this model in a generative fashion to create new sentences that have the highest probability of looking like Bukowski's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_index[start_symbol]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_index[stop_symbol]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [word_list[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
